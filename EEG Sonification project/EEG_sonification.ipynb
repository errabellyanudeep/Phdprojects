{
 "cells": [
  {
   "cell_type": "raw",
   "id": "20e7240d-7095-4311-97bd-2fabcd638d5e",
   "metadata": {},
   "source": [
    "#Medium Resource:- https://bagavanmm.medium.com/generating-music-with-my-brain-829d1c168ef8  \n",
    "\n",
    "#Github link :- https://github.com/BagavanMM/BrainwaveMusic-OpenBCI\n",
    "\n",
    "#Required Libraries : - numpy, pandas, matplotlib ,  MIDIUtil, mne  etc.. (Brainflow didnt use as of now good  for livedata)\n",
    "\n",
    "#for music note generation and conversion of MIDI file to the .mp3 file\n",
    "# pip install pygame\n",
    "# pip install pydub \n",
    "#pip install midi2audio\n",
    "\n",
    "#extra stop before \"pydub\" is    install it in command prompt of mac osx  \" brew install ffmpeg  \"  \n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Dataset used :- Electroencephalograms during Mental Arithmetic Task Performance\n",
    "\n",
    "#What Dataset contain:- EEG recordings of subjects before and during the performance of mental arithmetic tasks (Serial subtraction of two numbers)  e.g. (3141 - 42).\n",
    "\n",
    "#dataset distinguishments:-\n",
    "with \"_1\" suffix -- the recording of the background EEG of a subject (before mental arithmetic task)\n",
    "with \"_2\" suffix -- the recording of EEG during the mental arithmetic task.\n",
    "\n",
    "#Dataset  source:- physionet.org/content/eegmat/1.0.0/  (No restrictions, open source)\n",
    "\n",
    "#citation of Datset used:-  \n",
    "Zyma I, Tukaev S, Seleznov I, Kiyono K, Popov A, Chernykh M, Shpenkov O. Electroencephalograms during Mental Arithmetic Task Performance. Data. 2019; 4(1):14. https://doi.org/10.3390/data4010014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dc340-66d5-4788-b39b-997708aebdfe",
   "metadata": {},
   "source": [
    "### Before Running  below code, install Reequired Libraries below  , if missing do !pip install <pkg_name>\n",
    "### Dataset can be downloaded from physionet.org/content/eegmat/1.0.0/\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4735c5c-ae73-4d7f-b77f-c7d64e70031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install MIDIUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a47e2-fa53-422a-8314-e5111df8928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436c172-6971-4ac6-bce4-a441a482e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9519e-d77d-4dd3-a804-7ccba05c8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install midi2audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325b5a89-87a5-4368-a51b-29fcd3401117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/deep/Desktop/eeg-during-mental-arithmetic-tasks-1.0.0 2/Subject00_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 90999  =      0.000 ...   181.998 secs...\n",
      "----------------------------------------------------------------------------------------------------\n",
      " \n",
      "raw_channel names are below\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['EEG Fp1', 'EEG Fp2', 'EEG F3', 'EEG F4', 'EEG F7', 'EEG F8', 'EEG T3', 'EEG T4', 'EEG C3', 'EEG C4', 'EEG T5', 'EEG T6', 'EEG P3', 'EEG P4', 'EEG O1', 'EEG O2', 'EEG Fz', 'EEG Cz', 'EEG Pz', 'EEG A2-A1', 'ECG ECG']\n",
      " \n",
      "----------------------------------------------------------------------------------------------------\n",
      "renamed_channel names as per 10-20 are below\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2-A1', 'ECG']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finalized channels are as shown below\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz']\n"
     ]
    }
   ],
   "source": [
    "#importing Raw EDF File before (Mental Airthmetic Task)\n",
    "import mne\n",
    "\n",
    "#Helps to see interactive  raw EEG plots with time stamps inckuded clearly\n",
    "%matplotlib qt\n",
    "\n",
    "#Recording  before Mental Airthematic Task for subject_00.\n",
    "raw_eeg_1 = mne.io.read_raw_edf('/Users/deep/Desktop/eeg-during-mental-arithmetic-tasks-1.0.0 2/Subject00_1.edf',preload=True)\n",
    "\n",
    "print(\"-\"*100)\n",
    "raw_channels = raw_eeg_1.info['ch_names']\n",
    "print(\" \")\n",
    "print(\"raw_channel names are below\")\n",
    "print(\"-\"*100)\n",
    "print(raw_channels)\n",
    "\n",
    "\n",
    "# #Creating mapping from current channel names {keys} in dict below to the standard 10-20 names {values}\n",
    "# #in dict \"channel_mapping\n",
    "channel_map= {\n",
    "    'EEG Fp1': 'Fp1', 'EEG Fp2': 'Fp2', 'EEG F3': 'F3', 'EEG F4': 'F4',\n",
    "    'EEG F7': 'F7', 'EEG F8': 'F8', 'EEG T3': 'T3', 'EEG T4': 'T4',\n",
    "    'EEG C3': 'C3', 'EEG C4': 'C4', 'EEG T5': 'T5', 'EEG T6': 'T6',\n",
    "    'EEG P3': 'P3', 'EEG P4': 'P4', 'EEG O1': 'O1', 'EEG O2': 'O2',\n",
    "    'EEG Fz': 'Fz', 'EEG Cz': 'Cz', 'EEG Pz': 'Pz', 'EEG A2-A1': 'A2-A1',\n",
    "    'ECG ECG': 'ECG',\n",
    "}\n",
    "\n",
    "\n",
    "raw_eeg_1.rename_channels(channel_map)\n",
    "print(\" \")\n",
    "print(\"-\"*100)\n",
    "print(\"renamed_channel names as per 10-20 are below\")\n",
    "print(\"-\"*100)\n",
    "print(raw_eeg_1.info['ch_names'])\n",
    "\n",
    "\n",
    "#dropping  these as A2-A1 are mastoids just reference purpose only, ECG is not needed so dropped to match 10-20 standards\n",
    "raw_eeg_1.drop_channels(['A2-A1', 'ECG'])\n",
    "\n",
    "    \n",
    "# Get the raw EEG data as a NumPy array\n",
    "eeg_data_numpy_array = raw_eeg_1.get_data()\n",
    "print(\"-\"*100)\n",
    "\n",
    "#finalized channels taken into consideration after droping    'A2-A1', 'ECG'\n",
    "print(\"Finalized channels are as shown below\")\n",
    "print(raw_eeg_1.info['ch_names'])\n",
    "\n",
    "\n",
    "#Recording  during Mental Airthematic Task for same subject_00.\n",
    "# raw_eeg_2 = mne.io.read_raw_edf('/Users/deep/Desktop/eeg-during-mental-arithmetic-tasks-1.0.0 2/Subject00_2.edf',preload=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7efad95-b972-432f-8d41-2ca49dc64c7d",
   "metadata": {},
   "source": [
    "#Visualize the raw_eeg_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77eca53-4549-43ad-ada1-2072b7b98b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_eeg_1 = raw_eeg_1.plot(block=True,duration=10.0, title='eeg_After_AirthmeticTask_sub00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964851bc-4464-4669-af56-2d5f3f166cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m raw_eeg_1\u001b[38;5;241m.\u001b[39mplot_sensors(show_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Plot the sensor positions to verify\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# fig.suptitle('standard 10-20 system for  Channel Names', fontsize=12)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m fig\u001b[38;5;241m.\u001b[39msubplots_adjust(top\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fig' is not defined"
     ]
    }
   ],
   "source": [
    "#Makinng the channels to fit into standard montage of 10-20 as the researchers in paper didn't provide\n",
    "#Digitization or spatial locations of electrodes to have topographic or power spectral density plots\n",
    "\n",
    "# Load the standard 1005 montage\n",
    "montage = mne.channels.make_standard_montage('standard_1005')\n",
    "\n",
    "# Set the montage to the raw EEG data\n",
    "raw_eeg_1.set_montage(montage)\n",
    "\n",
    "# Now you can proceed with further analysis or visualization\n",
    "raw_eeg_1.plot_sensors(show_names=True)  # Plot the sensor positions to verify\n",
    "\n",
    "# fig.suptitle('standard 10-20 system for  Channel Names', fontsize=12)\n",
    "fig.subplots_adjust(top=0.9) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f65a69-dfa7-4e0e-8055-2424362ff89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n"
     ]
    }
   ],
   "source": [
    "#plotting topo_maps in different frequency Bands\n",
    "import matplotlib.pyplot as plt\n",
    "# freqbands =  [(3,7) ,(8,12), (13,30)]\n",
    "freqbands = [(0,4), (4, 8), (8, 12), (13, 30)]\n",
    "\n",
    "\n",
    "#subplot for each Freq bands\n",
    "fig , axes = plt.subplots(1 , 4 , figsize= (15, 5))   #1 row , 3 columns , 3 subplots wid size 15,5\n",
    "\n",
    "#looping over Freq bands individually\n",
    "for i, (fmin,fmax) in enumerate (freqbands):\n",
    "    ax = axes[i]\n",
    "    #computing PSD using Multi-taper method\n",
    "    psd , freqs  = mne.time_frequency.psd_array_multitaper(raw_eeg_1.get_data() ,  raw_eeg_1.info['sfreq'], fmin =fmin  , fmax=fmax)\n",
    "    #Visualization of topo_maps fpr all channels\n",
    "    mne.viz.plot_topomap(psd.mean(axis = -1), pos =raw_eeg_1.info , cmap='RdBu_r' , axes = ax, names=raw_eeg_1.info['ch_names'])\n",
    "    ax.set(title = 'PSD ({:.1f} - {:.1f})Hz' .format(fmin, fmax))\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efff8d9e-9cc3-4353-9d87-467693c12713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n"
     ]
    }
   ],
   "source": [
    "# ## Select desired channels\n",
    "# desired_channels = ['O1', 'O2','Cz']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea7529-d4b0-4af0-8854-f5a73d59986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selective_channels =  raw_eeg_1.pick_channels(desired_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f2991-9447-44e3-a14f-6c6afe2bbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check the selected channels\n",
    "# print(selective_channels.info['ch_names'])  # Should print ['Cz', 'O1', 'O2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed7bef-4423-48f3-a0e9-7eea1115945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting to know sampling rate used \n",
    "raw_eeg_1.info[\"sfreq\"]\n",
    "print(\"Sampling Frequency is  500Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2c4f4-cd61-47f7-a480-53bee84ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appraoch -1  To map eeg paramters to ADSR (attack, delay, sustain, reelase times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9e434-b96b-4296-af3b-c51364cbd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_channels = ['T4']\n",
    "raw_eeg_selected_from_O2channel = raw_eeg_1.pick_channels(desired_channels,ordered=False)\n",
    "raw_eeg_selected_from_O2channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adaeadf-b35b-49a4-b404-be6999f8c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pywt/_cwt.py:117: FutureWarning: Wavelets from the family cmor, without parameters specified in the name are deprecated. The name should takethe form cmorB-C where B and C are floats representing the bandwidth frequency and center frequency, respectively (example: cmor1.5-1.0).\n",
      "  wavelet = DiscreteContinuousWavelet(wavelet)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band Powers: {'delta': 8.302451717501637e-10, 'theta': 4.771673555656242e-10, 'alpha': 7.0809208396453e-10, 'beta': 3.3372407950628576e-10, 'gamma': 5.516465265531769e-11}\n",
      "Band Powers (dB): {'delta': -90.80793641340367, 'theta': -93.21329275359321, 'alpha': -91.49910260732264, 'beta': -94.76612456157855, 'gamma': -102.58339112015452}\n",
      "Most activated band: delta\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "# Load the EEG data (assuming raw_eeg_1 is already loaded)\n",
    "desired_channel = 'T4'\n",
    "raw_eeg_selected = raw_eeg_1.copy().pick_channels([desired_channel])\n",
    "eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# Define frequency bands and corresponding scales for CWT\n",
    "sfreq = raw_eeg_selected.info['sfreq']\n",
    "frequency_bands = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 12),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (30, 45)\n",
    "}\n",
    "\n",
    "# Function to perform CWT and get power in each band\n",
    "def compute_wavelet_power(data, sfreq, band, wavelet='cmor'):\n",
    "    low_freq, high_freq = band\n",
    "    scales = np.arange(1, 128)\n",
    "    coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "    power = (abs(coefficients)) ** 2\n",
    "    # Select coefficients within the desired frequency range\n",
    "    band_power = power[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    selected_coeffs = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    return band_power.mean(), band_power, selected_coeffs\n",
    "\n",
    "# Function to convert power to decibels\n",
    "def power_to_db(power):\n",
    "    return 10 * np.log10(power)\n",
    "\n",
    "# Dictionary to store power in each band\n",
    "band_powers = {}\n",
    "band_powers_db = {}\n",
    "wavelet_coeffs = {}\n",
    "\n",
    "# Compute the power in each frequency band and convert to dB\n",
    "for band, freq_range in frequency_bands.items():\n",
    "    power, band_power, coeffs = compute_wavelet_power(eeg_data, sfreq, freq_range)\n",
    "    band_powers[band] = power if power > 0 else 'Band Absent'\n",
    "    band_powers_db[band] = power_to_db(power) if power > 0 else 'Band Absent'\n",
    "    wavelet_coeffs[band] = coeffs if power > 0 else None\n",
    "\n",
    "# Find the most activated band in linear scale\n",
    "most_activated_band = max(band_powers, key=lambda k: band_powers[k] if isinstance(band_powers[k], (int, float)) else -1)\n",
    "print(\"Band Powers:\", band_powers)\n",
    "print(\"Band Powers (dB):\", band_powers_db)\n",
    "print(\"Most activated band:\", most_activated_band)\n",
    "\n",
    "# Visualize the power in each band\n",
    "fig, axes = plt.subplots(1, len(frequency_bands), figsize=(15, 5))\n",
    "fig.suptitle(f'Wavelet Power Spectral Density for {desired_channel}')\n",
    "\n",
    "for ax, (band, freq_range) in zip(axes, frequency_bands.items()):\n",
    "    power = band_powers[band]\n",
    "    coeffs = wavelet_coeffs[band]\n",
    "    if isinstance(power, str):  # Band absent\n",
    "        ax.text(0.5, 0.5, 'Band Absent', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.imshow(abs(coeffs), extent=[0, len(eeg_data)/sfreq, freq_range[0], freq_range[1]], aspect='auto', cmap='viridis')\n",
    "        ax.set_title(f'{band.capitalize()} ({freq_range[0]}-{freq_range[1]} Hz)')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decomposed waves\n",
    "fig, axes = plt.subplots(len(frequency_bands), 1, figsize=(15, 10))\n",
    "fig.suptitle(f'Decomposed Waves for {desired_channel}')\n",
    "\n",
    "for ax, (band, _) in zip(axes, frequency_bands.items()):\n",
    "    coeffs = wavelet_coeffs[band]\n",
    "    if coeffs is None:  # Band absent\n",
    "        ax.text(0.5, 0.5, 'Band Absent', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        # Sum coefficients over the frequency range to get the decomposed signal\n",
    "        decomposed_wave = np.sum(abs(coeffs), axis=0)\n",
    "        ax.plot(np.arange(len(decomposed_wave)) / sfreq, decomposed_wave)\n",
    "        ax.set_title(f'{band.capitalize()} Wave')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b563132-8d29-468e-b70b-bd1c48c83d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approch 3- to map eeg paramters into the ADSR formats for digital synthesizer.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "\n",
    "#Desired eeg channel \n",
    "eeg_data = eeg_data\n",
    "\n",
    "# Band-pass filter parameters  as we have seen most dominant band acvtive is Delta\n",
    "lowcut = 8.0\n",
    "highcut = 12.0\n",
    "fs = 500.0  # Sampling frequency\n",
    "\n",
    "# Design band-pass filter\n",
    "nyq = 0.5 * fs\n",
    "low = lowcut / nyq\n",
    "high = highcut / nyq\n",
    "b, a = signal.butter(4, [low, high], btype='band')\n",
    "\n",
    "\n",
    "# Apply band-pass filter\n",
    "filtered_eeg = signal.filtfilt(b, a, eeg_data)\n",
    "\n",
    "# Segment the EEG signal (for simplicity, we use the entire filtered signal)\n",
    "segment = filtered_eeg\n",
    "\n",
    "# Function to extract ADSR parameters\n",
    "def extract_adsr(segment):\n",
    "    peak_index = np.argmax(segment)\n",
    "    peak_value = segment[peak_index]\n",
    "\n",
    "    # Attack time: time to reach peak\n",
    "    attack_time = peak_index\n",
    "\n",
    "    # Decay time: time to reach a stable level after peak\n",
    "    stable_level = np.mean(segment[peak_index:])\n",
    "    print( stable_level)\n",
    "    decay_index = np.where(segment[peak_index:] <= stable_level)[0][0] + peak_index\n",
    "    decay_time = decay_index - peak_index\n",
    "\n",
    "    \n",
    "    \n",
    "    # Example range of stable_level (adjust with your actual observed data range)\n",
    "    min_level = np.min(stable_level)\n",
    "    max_level = np.max(stable_level)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    if max_level != min_level:\n",
    "        normalized_sustain_level = (stable_level - min_level) / (max_level - min_level)\n",
    "\n",
    "    else:\n",
    "    # Handle the case where max_level == min_level\n",
    "        normalized_sustain_level = 0 if stable_level == min_level else 1\n",
    "    \n",
    "    # Ensure it falls within [0, 1]\n",
    "    normalized_sustain_level = np.clip(normalized_sustain_level, 0, 1)\n",
    "\n",
    "    # Release time: time to return to baseline\n",
    "    release_index = len(segment) - 1\n",
    "    release_time = release_index - decay_index\n",
    "\n",
    "    return attack_time, decay_time, normalized_sustain_level, release_time\n",
    "\n",
    "# Extract ADSR parameters\n",
    "attack_time, decay_time, normalized_sustain_level, release_time = extract_adsr(segment)\n",
    "\n",
    "print(attack_time)\n",
    "print(f\"Attack Time: {attack_time/500.0}\")\n",
    "print(f\"Decay Time: {decay_time/500.0}\")\n",
    "print(f\"Sustain Level: {normalized_sustain_level}\")\n",
    "print(f\"Release Time: {release_time/500.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514900d-a001-49d0-9976-052d206df944",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 2.769461692463672e-10  # Your value to be rounded\n",
    "\n",
    "# Round off and constrain within [0, 1]\n",
    "rounded_value = max(0, min(1, round(value, 10)))  # Adjust the precision as needed (here rounded to 10 decimal places)\n",
    "\n",
    "print(rounded_value)  # Print or use the rounded value as required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a866b-6d7a-4e0a-b034-33a452e8630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract ADSR parameters\n",
    "def extract_adsr(segment, fs=500.0):\n",
    "    peak_index = np.argmax(segment)\n",
    "    peak_value = segment[peak_index]\n",
    "\n",
    "    # Attack time: time to reach peak\n",
    "    attack_time = peak_index / fs\n",
    "\n",
    "    # Decay time: time to reach a stable level after peak\n",
    "    stable_level = np.mean(segment[peak_index:])\n",
    "    decay_index = np.where(segment[peak_index:] <= stable_level)[0][0] + peak_index\n",
    "    decay_time = (decay_index - peak_index) / fs\n",
    "\n",
    "    # Normalize sustain level\n",
    "    min_level = np.min(segment)\n",
    "    max_level = np.max(segment)\n",
    "    \n",
    "    if max_level != min_level:\n",
    "        normalized_sustain_level = (stable_level - min_level) / (max_level - min_level)\n",
    "    else:\n",
    "        normalized_sustain_level = 0 if stable_level == min_level else 1\n",
    "    \n",
    "    normalized_sustain_level = np.clip(normalized_sustain_level, 0, 1)\n",
    "\n",
    "    # Release time: time to return to baseline\n",
    "    release_time = (len(segment) - decay_index) / fs\n",
    "\n",
    "    return attack_time, decay_time, normalized_sustain_level, release_time\n",
    "\n",
    "# Example usage:\n",
    "# Assuming segment is already defined and contains your filtered EEG data\n",
    "attack_time, decay_time, normalized_sustain_level, release_time = extract_adsr(segment)\n",
    "\n",
    "# Print results in seconds\n",
    "print(f\"Attack Time: {attack_time} seconds\")\n",
    "print(f\"Decay Time: {decay_time} seconds\")\n",
    "print(f\"Sustain Level: {normalized_sustain_level}\")\n",
    "print(f\"Release Time: {release_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdc58c-b284-4eb2-8caa-f3efd6a7e0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e6248-4450-4980-9216-81276a295403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end uisng tone .js approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a40b0b-0217-4273-9470-7417e7309474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb42553-239e-4af7-8581-45459e376d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df7112-a409-447b-94ea-2479fcd4fe86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09401a7-95bd-48b8-a855-022701141036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a99d8e-5b87-439e-bc77-256d2bdd0f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce2fa5-d105-448f-9cc8-f1cd4695fde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09263cd6-eadf-4370-94da-7bdf0bfb1175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b035b-2b6d-49d7-93c1-34797fa0997d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7885f-9899-4cfe-90b5-f96e150e4b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4acb4-1e52-4510-8e7b-193bf5fb5bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f40ab-65fe-4335-a0bc-189b58fcd5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43336594-febc-4d90-ae62-2f93e91da72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9d246-c5f7-4b33-939d-b6d534dee3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b4605-1907-4c11-89c7-6a741b2a1248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684b8d2-03b3-4353-bf58-4685757d4fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79f1d8-9c38-464e-9b38-6ab9b05d55e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd58cff-2c07-4fbf-a90d-fc26ae3b616a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da04bb8-f11e-4910-be02-c8910d5a5b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166e0fe-14db-489a-a4f9-cda57dd8416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from midiutil import MIDIFile\n",
    "\n",
    "# Select desired channels\n",
    "# desired_channels = ['Cz', 'O1', 'O2']\n",
    "\n",
    "desired_channels = ['Pz']\n",
    "raw_eeg_selected = raw_eeg_1.pick_channels(desired_channels,ordered=False)\n",
    "\n",
    "# List to store sampled data\n",
    "cycled_data = []\n",
    "\n",
    "# Iterate through data samples, sampling every 50th sample\n",
    "for idx in range(len(raw_eeg_selected._data[0])):\n",
    "    if idx % 50 == 0:\n",
    "        sample = raw_eeg_selected._data[:, idx]  # Select all channels at this time point\n",
    "        cycled_data.append(sample)\n",
    "\n",
    "cycled_data = np.array(cycled_data)\n",
    "print(\"Shape of cycled_data:\", cycled_data.shape)\n",
    "                       \n",
    "\n",
    "# List to store musical notes\n",
    "musical_notes = []\n",
    "\n",
    "# Convert EEG data into musical notes\n",
    "for sample in cycled_data:\n",
    "    for value in sample:\n",
    "        if 0 < value < 1:\n",
    "            musical_notes.append(60)\n",
    "        elif -1 < value < 0:\n",
    "            musical_notes.append(59)\n",
    "        elif -2 < value < -1:\n",
    "            musical_notes.append(58)\n",
    "        elif value < -2:\n",
    "            musical_notes.append(57)\n",
    "        elif 1 < value < 2:\n",
    "            musical_notes.append(62)\n",
    "        elif 2 < value < 3:\n",
    "            musical_notes.append(63)\n",
    "        elif 3 < value < 4:\n",
    "            musical_notes.append(64)\n",
    "        elif 4 < value < 5:\n",
    "            musical_notes.append(65)\n",
    "        elif value > 5:\n",
    "            musical_notes.append(66)\n",
    "\n",
    "print(\"Converting data into music notes\")\n",
    "\n",
    "# Create MIDI file\n",
    "track = 0\n",
    "channel = 0\n",
    "time_beat = 0   # In beats\n",
    "duration = 1   # In beats\n",
    "tempo = 250  # In BPM\n",
    "volume = 100 # 0-127, as per the MIDI standard\n",
    "\n",
    "MyMIDI = MIDIFile(1) # One track, defaults to format 1 (tempo track automatically created)\n",
    "MyMIDI.addTempo(track, time_beat, tempo)\n",
    "\n",
    "# Add notes to MIDI file\n",
    "for pitch in musical_notes:\n",
    "    MyMIDI.addNote(track, channel, pitch, time_beat, duration, volume)\n",
    "    time_beat += 1\n",
    "\n",
    "output_path = \"/Users/deep/Documents/OSU/PhD/experimental_results/eeg-music_j27_t1/results_before_airthematic_task/person-0/sub_01_pz.mid\"  # Change this to your desired path\n",
    "with open(output_path, \"wb\") as output_file:\n",
    "    MyMIDI.writeFile(output_file)\n",
    "\n",
    "print(f\"Conversion completed, file saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb24a8-6cdb-4a5d-9ed1-8bbc91bb5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sinethesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5f67f-0081-4fd5-b66a-3b216f8ceacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01396399-1707-4f91-93a6-c2651dc0aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33c3c3-33a2-4f9d-ae18-73c034578732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming raw_eeg_selected._data is your EEG data array\n",
    "# raw_eeg_selected._data.shape should be (n_channels, n_samples)\n",
    "desired_channels = ['O1']\n",
    "raw_eeg_selected = raw_eeg_1.pick_channels(desired_channels,ordered=False)\n",
    "\n",
    "# Select the first channel and the first 50 samples\n",
    "first_channel_first_50_samples = raw_eeg_selected._data[0, :50]\n",
    "\n",
    "# Print or further process the first 50 samples of the first channel\n",
    "print(\"First 50 samples of the first channel ('01'):\")\n",
    "print(first_channel_first_50_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad259d81-586b-45f9-9203-5c338535d42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb1753-4e7f-4b1b-9474-035b078884d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decomposed frequency bands of a particular channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d99c2-de5f-4613-99e9-4a6e085cdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "# Load the EEG data (assuming raw_eeg_1 is already loaded)\n",
    "desired_channel = 'O1'\n",
    "raw_eeg_selected = raw_eeg_1.copy().pick_channels([desired_channel])\n",
    "eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# Define frequency bands and corresponding scales for CWT\n",
    "sfreq = raw_eeg_selected.info['sfreq']\n",
    "frequency_bands = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 12),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (30, 45)\n",
    "}\n",
    "\n",
    "# Function to perform CWT and get power in each band\n",
    "def compute_wavelet_power(data, sfreq, band, wavelet='cmor'):\n",
    "    low_freq, high_freq = band\n",
    "    scales = np.arange(1, 128)\n",
    "    coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "    power = (abs(coefficients)) ** 2\n",
    "    # Select coefficients within the desired frequency range\n",
    "    band_power = power[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    selected_coeffs = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    return band_power.mean(), band_power, selected_coeffs\n",
    "\n",
    "# Dictionary to store power in each band\n",
    "band_powers = {}\n",
    "wavelet_coeffs = {}\n",
    "\n",
    "# Compute the power in each frequency band\n",
    "for band, freq_range in frequency_bands.items():\n",
    "    power, band_power, coeffs = compute_wavelet_power(eeg_data, sfreq, freq_range)\n",
    "    band_powers[band] = power if power > 0 else 'Band Absent'\n",
    "    wavelet_coeffs[band] = coeffs if power > 0 else None\n",
    "\n",
    "# Find the most activated band\n",
    "most_activated_band = max(band_powers, key=lambda k: band_powers[k] if isinstance(band_powers[k], (int, float)) else -1)\n",
    "print(\"Band Powers:\", band_powers)\n",
    "print(\"Most activated band:\", most_activated_band)\n",
    "\n",
    "# Visualize the power in each band\n",
    "fig, axes = plt.subplots(1, len(frequency_bands), figsize=(15, 5))\n",
    "fig.suptitle(f'Wavelet Power Spectral Density for {desired_channel}')\n",
    "\n",
    "for ax, (band, freq_range) in zip(axes, frequency_bands.items()):\n",
    "    power = band_powers[band]\n",
    "    coeffs = wavelet_coeffs[band]\n",
    "    if isinstance(power, str):  # Band absent\n",
    "        ax.text(0.5, 0.5, 'Band Absent', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.imshow(abs(coeffs), extent=[0, len(eeg_data)/sfreq, freq_range[0], freq_range[1]], aspect='auto', cmap='viridis')\n",
    "        ax.set_title(f'{band.capitalize()} ({freq_range[0]}-{freq_range[1]} Hz)')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decomposed waves\n",
    "fig, axes = plt.subplots(len(frequency_bands), 1, figsize=(15, 10))\n",
    "fig.suptitle(f'Decomposed Waves for {desired_channel}')\n",
    "\n",
    "for ax, (band, _) in zip(axes, frequency_bands.items()):\n",
    "    coeffs = wavelet_coeffs[band]\n",
    "    if coeffs is None:  # Band absent\n",
    "        ax.text(0.5, 0.5, 'Band Absent', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        # Sum coefficients over the frequency range to get the decomposed signal\n",
    "        decomposed_wave = np.sum(abs(coeffs), axis=0)\n",
    "        ax.plot(np.arange(len(decomposed_wave)) / sfreq, decomposed_wave)\n",
    "        ax.set_title(f'{band.capitalize()} Wave')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3572e7-fa88-4275-a875-1b43a1603afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f472aa-4473-40ec-9a54-51b579f52984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c72b3-dd12-4992-a0f8-38bfe3d37889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pywt\n",
    "# import mne\n",
    "# from midiutil import MIDIFile\n",
    "\n",
    "# # Load the EEG data (assuming raw_eeg_1 is already loaded)\n",
    "# desired_channels = ['O1']\n",
    "# raw_eeg_selected = raw_eeg_1.copy().pick_channels(desired_channels)\n",
    "# eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# # Define the sampling frequency and alpha band range\n",
    "# sfreq = raw_eeg_selected.info['sfreq']\n",
    "# alpha_band = (8, 12)\n",
    "\n",
    "# # Function to perform CWT and get power in the alpha band\n",
    "# def extract_alpha_band(data, sfreq, band=(8, 12), wavelet='cmor'):\n",
    "#     low_freq, high_freq = band\n",
    "#     scales = np.arange(1, 128)\n",
    "#     coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "#     # Select coefficients within the alpha band\n",
    "#     alpha_coefficients = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "#     alpha_signal = np.sum(alpha_coefficients, axis=0)\n",
    "#     return alpha_signal\n",
    "\n",
    "# # Extract the alpha band signal\n",
    "# alpha_signal = extract_alpha_band(eeg_data, sfreq, alpha_band)\n",
    "\n",
    "# # Normalize the alpha signal to the range [0, 1]\n",
    "# alpha_signal_normalized = (alpha_signal - np.min(alpha_signal)) / (np.max(alpha_signal) - np.min(alpha_signal))\n",
    "\n",
    "# # List to store sampled data\n",
    "# cycled_data = []\n",
    "\n",
    "# # Iterate through data samples, sampling every 50th sample\n",
    "# for idx in range(len(alpha_signal_normalized)):\n",
    "#     if idx % 50 == 0:\n",
    "#         sample = alpha_signal_normalized[idx]  # Select the alpha band signal at this time point\n",
    "#         cycled_data.append(sample)\n",
    "\n",
    "# cycled_data = np.array(cycled_data)\n",
    "# cycled_data = np.real(cycled_data)\n",
    "# print(\"Shape of cycled_data:\", cycled_data.shape)\n",
    "\n",
    "# # List to store musical notes\n",
    "# musical_notes = []\n",
    "\n",
    "# # Convert EEG data into musical notes\n",
    "# for value in cycled_data:\n",
    "#     if value < 0.1:\n",
    "#         musical_notes.append(57)  # A\n",
    "#     elif 0.1 <= value < 0.2:\n",
    "#         musical_notes.append(58)  # A#\n",
    "#     elif 0.2 <= value < 0.3:\n",
    "#         musical_notes.append(59)  # B\n",
    "#     elif 0.3 <= value < 0.4:\n",
    "#         musical_notes.append(60)  # C\n",
    "#     elif 0.4 <= value < 0.5:\n",
    "#         musical_notes.append(61)  # C#\n",
    "#     elif 0.5 <= value < 0.6:\n",
    "#         musical_notes.append(62)  # D\n",
    "#     elif 0.6 <= value < 0.7:\n",
    "#         musical_notes.append(63)  # D#\n",
    "#     elif 0.7 <= value < 0.8:\n",
    "#         musical_notes.append(64)  # E\n",
    "#     elif 0.8 <= value < 0.9:\n",
    "#         musical_notes.append(65)  # F\n",
    "#     else:\n",
    "#         musical_notes.append(66)  # F#\n",
    "\n",
    "# print(\"Converting data into music notes\")\n",
    "\n",
    "# # Create MIDI file\n",
    "# track = 0\n",
    "# channel = 0\n",
    "# time_beat = 0   # In beats\n",
    "# duration = 1   # In beats\n",
    "# tempo = 400  # In BPM\n",
    "# volume = 110 # 0-127, as per the MIDI standard\n",
    "\n",
    "# MyMIDI = MIDIFile(1) # One track, defaults to format 1 (tempo track automatically created)\n",
    "# MyMIDI.addTempo(track, time_beat, tempo)\n",
    "\n",
    "# # Add notes to MIDI file\n",
    "# for pitch in musical_notes:\n",
    "#     MyMIDI.addNote(track, channel, pitch, time_beat, duration, volume)\n",
    "#     time_beat += 1\n",
    "\n",
    "# # Save MIDI file\n",
    "# with open(\"01_bef.mid\", \"wb\") as output_file:\n",
    "#     MyMIDI.writeFile(output_file)\n",
    "\n",
    "# print(\"Conversion completed, play file alpha_band_music.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc8218-ff48-4a8a-9923-95cea8e2a242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee423f0-5f91-4e77-859b-2f8286ff4934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df00d10-a41e-49ca-a257-35876fcb8ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef49e3f-ca4e-478d-bbd3-88f62c282740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb447d3f-deba-487c-af4c-1b4216a8aea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab1263-165c-458c-a829-16f633b1189e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30055079-0e5b-4c37-8058-0718f3849be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycled_data = np.array(cycled_data)\n",
    "# print(cycled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac03d4-6f72-49d5-b1dd-ed8d40d61f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of cycled_data:\", cycled_data.shape)\n",
    "print(\"Min value:\", np.min(cycled_data))\n",
    "print(\"Max value:\", np.max(cycled_data))\n",
    "print(\"Mean value:\", np.mean(cycled_data))\n",
    "print(\"Standard deviation:\", np.std(cycled_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1a59c-c760-41d7-9a20-75fc0ed17945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e590851-b7c8-4d8b-a572-16a1bbef3389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8e829-e508-45b7-be9a-f2edc15484f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90214177-4479-4687-a285-e81263c8c5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f95f-15fa-4d3f-abfe-58b7e30bd723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752e912-cf25-460c-a69e-f134472b9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pywt\n",
    "# from midiutil import MIDIFile\n",
    "\n",
    "# # Select desired channels and extract data\n",
    "# desired_channels = ['O1']  # Example channel selection\n",
    "# raw_eeg_selected = raw_eeg_1.copy().pick_channels(desired_channels)\n",
    "# eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# # Define the sampling frequency and alpha band range\n",
    "# sfreq = raw_eeg_selected.info['sfreq']\n",
    "# alpha_band = (8, 12)\n",
    "\n",
    "# # Function to perform CWT and get power in the alpha band\n",
    "# def extract_alpha_band(data, sfreq, band=(8, 12), wavelet='cmor'):\n",
    "#     low_freq, high_freq = band\n",
    "#     scales = np.arange(1, 128)\n",
    "#     coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "    \n",
    "#     # Select coefficients within the alpha band\n",
    "#     alpha_coefficients = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    \n",
    "#     # Extract phase information (angle of complex coefficients)\n",
    "#     alpha_phase = np.angle(alpha_coefficients)\n",
    "    \n",
    "#     return alpha_coefficients, alpha_phase\n",
    "\n",
    "# # Extract the alpha band signal and phase\n",
    "# alpha_coefficients, alpha_phase = extract_alpha_band(eeg_data, sfreq, alpha_band)\n",
    "\n",
    "# # Calculate instantaneous frequency\n",
    "# delta_phase = np.diff(alpha_phase, axis=-1)\n",
    "# instantaneous_frequency = np.diff(np.unwrap(alpha_phase, axis=-1), axis=-1) / (2.0 * np.pi * (1.0 / sfreq))\n",
    "\n",
    "# # Calculate amplitude envelope\n",
    "# amplitude_envelope = np.abs(alpha_coefficients)\n",
    "\n",
    "# # Normalize amplitude envelope to [0, 1]\n",
    "# amplitude_normalized = (amplitude_envelope - np.min(amplitude_envelope)) / (np.max(amplitude_envelope) - np.min(amplitude_envelope))\n",
    "\n",
    "# # List to store MIDI notes and velocities\n",
    "# midi_notes = []\n",
    "# midi_velocities = []\n",
    "\n",
    "# # Convert EEG data into musical notes with dynamic velocity\n",
    "# for freq, amp in zip(instantaneous_frequency[0], amplitude_normalized):\n",
    "#     pitch = int(freq * 12) + 60  # Adjust scaling to fit MIDI pitch range\n",
    "    \n",
    "#     # Scale amplitude to MIDI velocity range for each instantaneous value\n",
    "#     for value in amp:\n",
    "#         velocity = int(value * 127)  # Scale amplitude to MIDI velocity range\n",
    "#         midi_notes.append(pitch)\n",
    "#         midi_velocities.append(velocity)\n",
    "\n",
    "# # Create MIDI file\n",
    "# track = 0\n",
    "# channel = 0\n",
    "# time_beat = 0   # In beats\n",
    "# duration = 1   # In beats\n",
    "# tempo = 120     # In BPM (adjust as needed)\n",
    "# volume = 110    # 0-127, as per the MIDI standard\n",
    "\n",
    "# MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track automatically created)\n",
    "# MyMIDI.addTempo(track, time_beat, tempo)\n",
    "\n",
    "# # Add notes to MIDI file\n",
    "# for pitch, velocity in zip(midi_notes, midi_velocities):\n",
    "#     MyMIDI.addNote(track, channel, pitch, time_beat, duration, velocity)\n",
    "#     time_beat += 1\n",
    "\n",
    "# # Save MIDI file\n",
    "# midi_filename = \"alpha_band_music.mid\"\n",
    "# with open(midi_filename, \"wb\") as output_file:\n",
    "#     MyMIDI.writeFile(output_file)\n",
    "\n",
    "# print(f\"Conversion completed, saved as {midi_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef507d-bb0a-4afe-81b9-bc65a3470154",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #finalli\n",
    "\n",
    "# import numpy as np\n",
    "# import pywt\n",
    "# from midiutil import MIDIFile\n",
    "\n",
    "# # Select desired channels and extract data\n",
    "# desired_channels = ['O1']  # Example channel selection\n",
    "# raw_eeg_selected = raw_eeg_1.copy().pick_channels(desired_channels)\n",
    "# eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# # Define the sampling frequency and alpha band range\n",
    "# sfreq = raw_eeg_selected.info['sfreq']\n",
    "# alpha_band = (8, 12)\n",
    "\n",
    "# # Function to perform CWT and get power in the alpha band\n",
    "# def extract_alpha_band(data, sfreq, band=(8, 12), wavelet='cmor'):\n",
    "#     low_freq, high_freq = band\n",
    "#     scales = np.arange(1, 128)\n",
    "#     coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "    \n",
    "#     # Select coefficients within the alpha band\n",
    "#     alpha_coefficients = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    \n",
    "#     # Extract phase information (angle of complex coefficients)\n",
    "#     alpha_phase = np.angle(alpha_coefficients)\n",
    "    \n",
    "#     return alpha_coefficients, alpha_phase\n",
    "\n",
    "# # Extract the alpha band signal and phase\n",
    "# alpha_coefficients, alpha_phase = extract_alpha_band(eeg_data, sfreq, alpha_band)\n",
    "\n",
    "# # Calculate instantaneous frequency\n",
    "# delta_phase = np.diff(alpha_phase, axis=-1)\n",
    "# instantaneous_frequency = np.diff(np.unwrap(alpha_phase, axis=-1), axis=-1) / (2.0 * np.pi * (1.0 / sfreq))\n",
    "\n",
    "# # Calculate amplitude envelope\n",
    "# amplitude_envelope = np.abs(alpha_coefficients)\n",
    "\n",
    "# # Normalize amplitude envelope to [0, 1]\n",
    "# amplitude_normalized = (amplitude_envelope - np.min(amplitude_envelope)) / (np.max(amplitude_envelope) - np.min(amplitude_envelope))\n",
    "\n",
    "# # Define a smaller segment length for faster processing\n",
    "# segment_length = 10  # Adjust as needed based on your data size and processing speed\n",
    "\n",
    "# # List to store MIDI notes and velocities\n",
    "# midi_notes = []\n",
    "# midi_velocities = []\n",
    "\n",
    "# # Process EEG data in segments\n",
    "# for start_idx in range(0, len(instantaneous_frequency[0]), segment_length):\n",
    "#     end_idx = min(start_idx + segment_length, len(instantaneous_frequency[0]))\n",
    "#     segment_freq = instantaneous_frequency[0][start_idx:end_idx]\n",
    "#     segment_amp = amplitude_normalized[start_idx:end_idx]\n",
    "    \n",
    "#     # Convert EEG data into musical notes with dynamic velocity\n",
    "#     for freq, amp in zip(segment_freq, segment_amp):\n",
    "#         pitch = int(freq * 12) + 60  # Adjust scaling to fit MIDI pitch range\n",
    "        \n",
    "#         # Scale amplitude to MIDI velocity range for each instantaneous value\n",
    "#         for value in amp:\n",
    "#             velocity = int(value * 127)  # Scale amplitude to MIDI velocity range\n",
    "#             midi_notes.append(pitch)\n",
    "#             midi_velocities.append(velocity)\n",
    "\n",
    "# # Create MIDI file\n",
    "# track = 0\n",
    "# channel = 0\n",
    "# time_beat = 0   # In beats\n",
    "# duration = 1   # In beats\n",
    "# tempo = 120     # In BPM (adjust as needed)\n",
    "# volume = 110    # 0-127, as per the MIDI standard\n",
    "\n",
    "# MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track automatically created)\n",
    "# MyMIDI.addTempo(track, time_beat, tempo)\n",
    "\n",
    "# # Add notes to MIDI file in batches\n",
    "# for pitch, velocity in zip(midi_notes, midi_velocities):\n",
    "#     MyMIDI.addNote(track, channel, pitch, time_beat, duration, velocity)\n",
    "#     time_beat += 1\n",
    "\n",
    "# # Save MIDI file\n",
    "# midi_filename = \"alpha_band_music.mid\"\n",
    "# with open(midi_filename, \"wb\") as output_file:\n",
    "#     MyMIDI.writeFile(output_file)\n",
    "\n",
    "# print(f\"Conversion completed, saved as {midi_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76c5e5-a4fe-48a1-8184-e12a4d500ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pywt\n",
    "# from midiutil import MIDIFile\n",
    "\n",
    "\n",
    "# # Select desired channels and extract data\n",
    "# desired_channels = ['O2']  # Example channel selection\n",
    "# raw_eeg_selected = raw_eeg_1.copy().pick_channels(desired_channels)\n",
    "# eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# # Define the sampling frequency and alpha band range\n",
    "# sfreq = raw_eeg_selected.info['sfreq']\n",
    "# alpha_band = (8, 12)\n",
    "\n",
    "# # Function to perform CWT and get power in the alpha band\n",
    "# def extract_alpha_band(data, sfreq, band=(8, 12), wavelet='cmor'):\n",
    "#     low_freq, high_freq = band\n",
    "#     scales = np.arange(1, 128)\n",
    "#     coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "    \n",
    "#     # Select coefficients within the alpha band\n",
    "#     alpha_coefficients = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    \n",
    "#     # Extract phase information (angle of complex coefficients)\n",
    "#     alpha_phase = np.angle(alpha_coefficients)\n",
    "    \n",
    "#     return alpha_coefficients, alpha_phase\n",
    "\n",
    "# # Extract the alpha band signal and phase\n",
    "# alpha_coefficients, alpha_phase = extract_alpha_band(eeg_data, sfreq, alpha_band)\n",
    "\n",
    "# # Calculate instantaneous frequency\n",
    "# delta_phase = np.diff(alpha_phase, axis=-1)\n",
    "# instantaneous_frequency = np.diff(np.unwrap(alpha_phase, axis=-1), axis=-1) / (2.0 * np.pi * (1.0 / sfreq))\n",
    "\n",
    "# # Calculate amplitude envelope\n",
    "# amplitude_envelope = np.abs(alpha_coefficients)\n",
    "\n",
    "# # Normalize amplitude envelope to [0, 1]\n",
    "# amplitude_normalized = (amplitude_envelope - np.min(amplitude_envelope)) / (np.max(amplitude_envelope) - np.min(amplitude_envelope))\n",
    "\n",
    "# # Duration to process (30 seconds)\n",
    "# duration_seconds = 30\n",
    "# num_samples_to_process = int(sfreq * duration_seconds)\n",
    "\n",
    "# # Limit to the first 30 seconds of data\n",
    "# eeg_data_segment = eeg_data[:num_samples_to_process]\n",
    "\n",
    "# # Recompute the alpha band coefficients and phase for the segment\n",
    "# alpha_coefficients_segment, alpha_phase_segment = extract_alpha_band(eeg_data_segment, sfreq, alpha_band)\n",
    "\n",
    "# # Recompute instantaneous frequency for the segment\n",
    "# delta_phase_segment = np.diff(alpha_phase_segment, axis=-1)\n",
    "# instantaneous_frequency_segment = np.diff(np.unwrap(alpha_phase_segment, axis=-1), axis=-1) / (2.0 * np.pi * (1.0 / sfreq))\n",
    "\n",
    "# # Recompute amplitude envelope for the segment\n",
    "# amplitude_envelope_segment = np.abs(alpha_coefficients_segment)\n",
    "\n",
    "# # Normalize amplitude envelope to [0, 1]\n",
    "# amplitude_normalized_segment = (amplitude_envelope_segment - np.min(amplitude_envelope_segment)) / (np.max(amplitude_envelope_segment) - np.min(amplitude_envelope_segment))\n",
    "\n",
    "# # Define a smaller segment length for faster processing\n",
    "# segment_length = 10  # Further reduce segment length\n",
    "\n",
    "# # Limit the number of samples processed for demo purposes\n",
    "# max_samples = 100  # Adjust as needed\n",
    "\n",
    "\n",
    "# # List to store MIDI notes and velocities\n",
    "# midi_notes = []\n",
    "# midi_velocities = []\n",
    "\n",
    "# # Process EEG data in segments\n",
    "# idx = 0\n",
    "# while idx < len(instantaneous_frequency_segment) and len(midi_notes) < max_samples:\n",
    "#     segment_freq = instantaneous_frequency_segment[idx:idx+segment_length]\n",
    "#     segment_amp = amplitude_normalized_segment[idx:idx+segment_length]\n",
    "    \n",
    "#     # Convert EEG data into musical notes with dynamic velocity\n",
    "#     for freq_array, amp_array in zip(segment_freq, segment_amp):\n",
    "#         for freq, amp in zip(np.nditer(freq_array), np.nditer(amp_array)):\n",
    "#             pitch = int(freq * 12) + 85  # Adjust scaling to fit MIDI pitch range\n",
    "#             pitch = min(max(pitch, 0), 127)  # Ensure pitch is within MIDI range\n",
    "            \n",
    "#             # Scale amplitude to MIDI velocity range for each instantaneous value\n",
    "#             velocity = int(amp * 127)  # Scale amplitude to MIDI velocity range\n",
    "            \n",
    "#             # Ensure velocity is within MIDI range (0 to 127)\n",
    "#             velocity = min(max(velocity, 0), 127)\n",
    "            \n",
    "#             midi_notes.append(int(pitch))  # Cast pitch to int explicitly\n",
    "#             midi_velocities.append(int(velocity))  # Cast velocity to int explicitly\n",
    "    \n",
    "#     idx += segment_length\n",
    "\n",
    "# # Create MIDI file\n",
    "# track = 0\n",
    "# channel = 0\n",
    "# time_beat = 0   # In beats\n",
    "# duration = 1   # In beats\n",
    "# tempo = 250    # In BPM (adjust as needed)\n",
    "# volume = 120    # 0-127, as per the MIDI standard\n",
    "\n",
    "# MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track automatically created)\n",
    "# MyMIDI.addTempo(track, time_beat, tempo)\n",
    "\n",
    "# # Add notes to MIDI file in batches\n",
    "# for pitch, velocity in zip(midi_notes, midi_velocities):\n",
    "#     MyMIDI.addNote(track, channel, pitch, time_beat, duration, velocity)\n",
    "#     time_beat += 1\n",
    "\n",
    "# # Save MIDI file\n",
    "# midi_filename = \"alpha_band_02_before.mid\"\n",
    "# with open(midi_filename, \"wb\") as output_file:\n",
    "#     MyMIDI.writeFile(output_file)\n",
    "\n",
    "# print(f\"Conversion completed, saved as {midi_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e17b78-f0a1-4d9c-ae5b-291a658dece7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619efdbf-853a-4358-84b7-cc771c9d2538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9eeb4-5c0d-4cf0-bffe-1abc597a918b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e3a4d-5080-4754-830f-769ce0135b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae38c91-2e05-4204-b942-580797b1c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pywt\n",
    "# from midiutil import MIDIFile\n",
    "\n",
    "# # Select desired channels and extract data\n",
    "# desired_channels = ['O2']  # Example channel selection\n",
    "# raw_eeg_selected = raw_eeg_1.copy().pick_channels(desired_channels)\n",
    "# eeg_data = raw_eeg_selected.get_data()[0]  # Get data for the selected channel\n",
    "\n",
    "# # Define the sampling frequency and alpha band range\n",
    "# sfreq = raw_eeg_selected.info['sfreq']\n",
    "# alpha_band = (8, 12)\n",
    "\n",
    "# # Function to perform CWT and get power in the alpha band\n",
    "# def extract_alpha_band(data, sfreq, band=(8, 12), wavelet='cmor'):\n",
    "#     low_freq, high_freq = band\n",
    "#     scales = np.arange(1, 128)\n",
    "#     coefficients, frequencies = pywt.cwt(data, scales, wavelet, 1.0/sfreq)\n",
    "    \n",
    "#     # Select coefficients within the alpha band\n",
    "#     alpha_coefficients = coefficients[(frequencies >= low_freq) & (frequencies <= high_freq)]\n",
    "    \n",
    "#     # Extract phase information (angle of complex coefficients)\n",
    "#     alpha_phase = np.angle(alpha_coefficients)\n",
    "    \n",
    "#     return alpha_coefficients, alpha_phase\n",
    "\n",
    "# # Extract the alpha band signal and phase\n",
    "# alpha_coefficients, alpha_phase = extract_alpha_band(eeg_data, sfreq, alpha_band)\n",
    "\n",
    "# # Calculate instantaneous frequency\n",
    "# delta_phase = np.diff(alpha_phase, axis=-1)\n",
    "# instantaneous_frequency = np.diff(np.unwrap(alpha_phase, axis=-1), axis=-1) / (2.0 * np.pi * (1.0 / sfreq))\n",
    "\n",
    "# # Calculate amplitude envelope\n",
    "# amplitude_envelope = np.abs(alpha_coefficients)\n",
    "\n",
    "# # Normalize amplitude envelope to [0, 1]\n",
    "# amplitude_normalized = (amplitude_envelope - np.min(amplitude_envelope)) / (np.max(amplitude_envelope) - np.min(amplitude_envelope))\n",
    "\n",
    "# # Duration to process (30 seconds)\n",
    "# duration_seconds = 30\n",
    "# num_samples_to_process = int(sfreq * duration_seconds)\n",
    "\n",
    "# # Limit to the first 30 seconds of data\n",
    "# eeg_data_segment = eeg_data[:num_samples_to_process]\n",
    "\n",
    "# # Recompute the alpha band coefficients and phase for the segment\n",
    "# alpha_coefficients_segment, alpha_phase_segment = extract_alpha_band(eeg_data_segment, sfreq, alpha_band)\n",
    "\n",
    "# # Recompute instantaneous frequency for the segment\n",
    "# delta_phase_segment = np.diff(alpha_phase_segment, axis=-1)\n",
    "# instantaneous_frequency_segment = np.diff(np.unwrap(alpha_phase_segment, axis=-1), axis=-1) / (2.0 * np.pi * (1.0 / sfreq))\n",
    "\n",
    "# # Recompute amplitude envelope for the segment\n",
    "# amplitude_envelope_segment = np.abs(alpha_coefficients_segment)\n",
    "\n",
    "# # Normalize amplitude envelope to [0, 1]\n",
    "# amplitude_normalized_segment = (amplitude_envelope_segment - np.min(amplitude_envelope_segment)) / (np.max(amplitude_envelope_segment) - np.min(amplitude_envelope_segment))\n",
    "\n",
    "# # Define a smaller segment length for faster processing\n",
    "# segment_length = 10  # Further reduce segment length\n",
    "\n",
    "# # Limit the number of samples processed for demo purposes\n",
    "# max_samples = 100  # Adjust as needed\n",
    "\n",
    "# # Function to map frequency to MIDI note in C major scale\n",
    "# def freq_to_midi_note(freq, scale='C major'):\n",
    "#     if scale == 'C major':\n",
    "#         # Define MIDI note numbers for C major scale (adjust octave as needed)\n",
    "#         c_major_scale = [60, 62, 64, 65, 67, 69, 71]  # MIDI notes for C4 to B4\n",
    "        \n",
    "#         # Map frequency to nearest MIDI note in C major scale\n",
    "#         note_index = int(np.round((freq - alpha_band[0]) / (alpha_band[1] - alpha_band[0]) * (len(c_major_scale) - 1)))\n",
    "        \n",
    "#         # Ensure note_index is within valid range\n",
    "#         if note_index < 0:\n",
    "#             note_index = 0\n",
    "#         elif note_index >= len(c_major_scale):\n",
    "#             note_index = len(c_major_scale) - 1\n",
    "        \n",
    "#         midi_note = c_major_scale[note_index]\n",
    "        \n",
    "#         return midi_note\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported scale: {scale}\")\n",
    "\n",
    "# # List to store MIDI notes and velocities\n",
    "# midi_notes = []\n",
    "# midi_velocities = []\n",
    "\n",
    "# # Process EEG data in segments\n",
    "# idx = 0\n",
    "# while idx < len(instantaneous_frequency_segment) and len(midi_notes) < max_samples:\n",
    "#     segment_freq = instantaneous_frequency_segment[idx:idx+segment_length]\n",
    "#     segment_amp = amplitude_normalized_segment[idx:idx+segment_length]\n",
    "    \n",
    "#     # Convert EEG data into musical notes with dynamic velocity\n",
    "#     for freq_array, amp_array in zip(segment_freq, segment_amp):\n",
    "#         for freq, amp in zip(np.nditer(freq_array), np.nditer(amp_array)):\n",
    "#             # Map frequency to MIDI note in C major scale\n",
    "#             pitch = freq_to_midi_note(freq)\n",
    "            \n",
    "#             # Scale amplitude to MIDI velocity range for each instantaneous value\n",
    "#             velocity = int(amp * 127)  # Scale amplitude to MIDI velocity range\n",
    "#             velocity = min(max(velocity, 0), 127)  # Ensure velocity is within MIDI range\n",
    "            \n",
    "#             midi_notes.append(int(pitch))\n",
    "#             midi_velocities.append(int(velocity))\n",
    "    \n",
    "#     idx += segment_length\n",
    "\n",
    "# # Create MIDI file\n",
    "# track = 0\n",
    "# channel = 0\n",
    "# time_beat = 0   # In beats\n",
    "# duration = 1   # In beats\n",
    "# tempo = 250    # Initial BPM (adjust as needed)\n",
    "# volume = 120    # 0-127, as per the MIDI standard\n",
    "\n",
    "# MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track automatically created)\n",
    "# MyMIDI.addTempo(track, time_beat, tempo)\n",
    "\n",
    "# # Add notes to MIDI file in batches\n",
    "# for pitch, velocity in zip(midi_notes, midi_velocities):\n",
    "#     MyMIDI.addNote(track, channel, pitch, time_beat, duration, velocity)\n",
    "#     time_beat += 1\n",
    "\n",
    "# # Save MIDI file\n",
    "# midi_filename = \"tiO2.mid\"\n",
    "# with open(midi_filename, \"wb\") as output_file:\n",
    "#     MyMIDI.writeFile(output_file)\n",
    "\n",
    "# print(f\"Conversion completed, saved as {midi_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdc086-9cae-4de7-9da5-c18eeec20ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0d841-99e1-41cc-94f1-6a4e3d4dcf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90865784-6b56-41dd-8c51-e946e3a31dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c2d67-07e5-44f2-b8e1-ed7b0e415044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from midiutil import MIDIFile\n",
    "# import mido\n",
    "\n",
    "# # Function to extract note sequences from a MIDI file\n",
    "# def extract_note_sequences(filename):\n",
    "#     mid = mido.MidiFile(filename)\n",
    "#     note_sequence = []\n",
    "    \n",
    "#     for msg in mid:\n",
    "#         if msg.type == 'note_on':\n",
    "#             note_sequence.append(msg.note)\n",
    "    \n",
    "#     return note_sequence\n",
    "\n",
    "# # Define MIDI filenames\n",
    "# midi_files = [\"ticz.mid\", \"tiO2.mid\"]\n",
    "\n",
    "# # Extract note sequences from MIDI files\n",
    "# note_sequences = []\n",
    "# for filename in midi_files:\n",
    "#     note_sequence = extract_note_sequences(filename)\n",
    "#     note_sequences.append(note_sequence)\n",
    "\n",
    "# # Function to calculate similarity score (example using simple list comparison)\n",
    "# def calculate_similarity(sequence1, sequence2):\n",
    "#     common_elements = set(sequence1) & set(sequence2)\n",
    "#     similarity_score = len(common_elements) / max(len(sequence1), len(sequence2))\n",
    "#     return similarity_score\n",
    "\n",
    "# # Compare MIDI files\n",
    "# for i in range(len(midi_files)):\n",
    "#     for j in range(i + 1, len(midi_files)):\n",
    "#         score = calculate_similarity(note_sequences[i], note_sequences[j])\n",
    "#         print(f\"Similarity score between {midi_files[i]} and {midi_files[j]}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d5913-8365-4399-8c80-9e20af9d3a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
